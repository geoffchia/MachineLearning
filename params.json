{"name":"Machinelearning","tagline":"Coursera Machine Learning Project","body":"---\r\ntitle: \"Predicting Exercise Manner using Random Forest\"\r\nauthor: \"geoffchia\"\r\ndate: \"Sunday, April 26, 2015\"\r\noutput: html_document\r\n---\r\n\r\n### Objective\r\n\r\nThe objective of this project is to build a predictive model using machine learning method to correctly predict how people perform their exercises by classifying them into one of the 5 categories: A, B, C, D and E.\r\n\r\n### Data\r\nThe training data for this project are provided and can be found here:\r\n[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)\r\n\r\nThe test data are also made available:\r\n[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)\r\n\r\n### Data Processing and Cleaning\r\nFirst we download both data files to the local folder: c:/Coursera. We then use R to load the data and to take a quick glance at the data\r\n\r\n```{r}\r\nlibrary(caret); library(data.table); library(randomForest)\r\nsetwd(\"C://Coursera\")\r\ndat <- read.csv(\"pml-training.csv\", na.strings=\"NA\")\r\ndim(dat)\r\n```\r\n\r\nWe then get rid of useless columns (e.g. those with a lot of NAs, or blanks), and only retain the predictors which are those with column name containing \"belt\", \"arm\" and \"dumbbell\"\r\n\r\n```{r}\r\n\r\n# get rid of columns with a lot of na values\r\ndat <- dat[colSums(is.na(dat)) < 1000]\r\n\r\n# exclude columns where most values are \"\"\r\ncols <- c()\r\nfor (cname in colnames(dat))\r\n    if (sum(dat[, cname] == \"\") < 1000) {\r\n        cols <- c(cols, cname)\r\n    }\r\ndat <- dat[, cols]\r\n\r\n# only retain columns with names consisting of \"belt\", \"arm\", \"dumbbell\"\r\ncols <- grep(\"belt\", colnames(dat))\r\ncols <- c(cols, grep(\"arm\", colnames(dat)))\r\ncols <- c(cols, grep(\"dumbbell\", colnames(dat)))\r\n\r\n# add back \"classe\", which is the last col\r\ncols <- c(cols, dim(dat)[2])\r\ndat <- dat[, cols]\r\ndim(dat)\r\n```\r\n\r\nAs can be seen, the predictors have been reduced from 160 to 53, a more manageable number for modeling.\r\n\r\n### Training and Testing Data\r\nWe then sub-divide the data to 75% training and 25% testing. The purpose is for us to calculate out-of-sample error later.\r\n\r\n```{r}\r\ninTrain <- createDataPartition(dat$classe, p=.75, list=FALSE)\r\ntraining <- dat[inTrain,]\r\ntesting <- dat[-inTrain,]\r\n```\r\n\r\n### Building Random Forest Model\r\nWe choose Random Forest model because it is one of the more powerful and commonly used machine learning model. We first use all the default settings. \r\n\r\n```{r, cache=TRUE}\r\nset.seed(23221)\r\nrf <- randomForest(classe ~., data=training)\r\nrf\r\n```\r\n\r\n### Calculate Out-of-Sample Error\r\nTo assess the performance of the model, we apply it to make prediction on the testing data and work out the out-of-sample error:\r\n\r\n```{r, cache=TRUE}\r\n# make prediction on testing data using our model\r\npred <- predict(rf, testing)\r\n\r\n# calculate out-of-sample error\r\ntbl <- table(pred, testing$classe)\r\nerr <- 1 - sum(diag(tbl)) / sum(tbl)\r\nerr\r\n```\r\nThe model performs quite well, so there is no need to tweak the parameters further.\r\n\r\n### Putting Our Model to Work\r\nWe now use our model to predict the 20 cases in the test data:\r\n\r\nNote: codes omitted to comply with Code of Honour of Coursera.\r\n\r\n### Conclusion\r\nIn this simple exercise, the default random forest model proves to be a fairly suitable model and we do not need to perform other tweaking. In practice, we would normally require to explore various models before deciding on the final one.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}